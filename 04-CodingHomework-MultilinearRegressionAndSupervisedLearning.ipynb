{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5327659",
   "metadata": {},
   "source": [
    "# Multilinear Regression and Supervised Learning\n",
    "\n",
    "In this assignment, you will explore the regression and classification models that we discussed in class. While the class discussion was focused on theory, the focus here is on applying the methods to data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415d060",
   "metadata": {},
   "source": [
    "We'll use a couple of new packages for loading and manipulating data:\n",
    " - `seaborn` is mostly for visualization of statistical analyses of data, but has some useful datasets that can be loaded from the package\n",
    " - `pandas` is a useful tool for working with spreadsheet-style data (technically, the data type used here is called a \"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1774ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a642766",
   "metadata": {},
   "source": [
    "## Multilinear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bfb23d",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b511f00",
   "metadata": {},
   "source": [
    "We'll load a dataset containing various statistics about car models, including their typical miles-per-gallon (mpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data = sns.load_dataset(\"mpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d4ad2",
   "metadata": {},
   "source": [
    "To get an idea for what the data looks like, we can look at the first few lines using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b72496",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bfe281",
   "metadata": {},
   "source": [
    "We can see that there are 398 data samples with 9 pieces of information per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b94aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e60ed3",
   "metadata": {},
   "source": [
    "However, if we look through the data, we see that some entries in the 'horsepower' column are not numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(mpg_data['horsepower'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a2102",
   "metadata": {},
   "source": [
    "We'll remove any rows from our data where `nan` appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data = mpg_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51834000",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a2c55",
   "metadata": {},
   "source": [
    "Now let's fit this into the regression framework that we described in class. \n",
    "\n",
    "We will train our model to predict mpg from the remaining numerical variables (cylinders, displacement, horsepower, weight, acceleration, model_year). This means that our data matrix should be of the form\n",
    "$$\n",
    "X = \\left[\\begin{array}{c}\n",
    "\\vec{x}_1^T \\\\\n",
    "\\vdots \\\\\n",
    "\\vec{x}_{398}^T \\end{array}\\right] \\in \\mathbb{R}^{398 \\times 6},\n",
    "$$\n",
    "where each $\\vec{x}_i$ is a vector of 6 numerical measurements for car model $i$. The vector we are trying to predict is of the form\n",
    "$$\n",
    "\\vec{y} = \\left[\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_{398}\\end{array}\\right] \\in \\mathbb{R}^{398},\n",
    "$$\n",
    "where each $y_i$ is the mpg for care model $i$. \n",
    "\n",
    "Let's set these up as `numpy` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54650788",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(mpg_data[['cylinders','displacement','horsepower','weight','acceleration','model_year']])\n",
    "y = np.array(mpg_data['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a774ad",
   "metadata": {},
   "source": [
    "### Building a Multilinear Regression Model\n",
    "\n",
    "Recall the general setup for multilinear regression: give a data matrix \n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "\\vec{x}_1^T \\\\\n",
    "\\vec{x}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\vec{x}_N^T\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{N \\times d}\n",
    "$$\n",
    "with $N$ samples and $d$ features, and a vector of observations \n",
    "$$\n",
    "\\vec{y} = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix}\\in \\mathbb{R}^N,\n",
    "$$\n",
    "our goal is to find parameters $\\beta_0,\\beta_1,\\ldots,\\beta_d$ which minimize the loss function\n",
    "$$\n",
    "L(\\beta_0,\\ldots,\\beta_d) = \\sum_{i=1}^N \\left( y_i - (\\beta_0 + \\beta_1 (\\vec{x}_i)_1 + \\cdots + \\beta_d (\\vec{x}_i)_d) \\right)^2.\n",
    "$$\n",
    "\n",
    "We will now build our model, which means finding the best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e46cb",
   "metadata": {},
   "source": [
    "### Adding Bias\n",
    "\n",
    "To incorporate the $\\beta_0$ term (this is sometimes referred to in machine learning as the *bias*) in the model, we need to do is add a column of all ones to our data matrix $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92901a1a",
   "metadata": {},
   "source": [
    "**Problem 1.** Fix the data matrix `X` defined above by adding a column of ones. Do this so that the *last* column contains ones. That is, the current $X \\in \\mathbb{R}^{398 \\times 6}$ should be replaced by a new $X \\in \\mathbb{R}^{398 \\times 7}$ whose first six columns contains come from the original $X$ and whose last column contains all ones.\n",
    "\n",
    "**Note:** Code below will assume that your new data matrix is still called `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a11558",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e767acc",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec3122",
   "metadata": {},
   "source": [
    "When training our model (i.e., solving for the optimal parameters), we will only use a subset of the given data---this is the *training data*. The resulting model can then be tested on the remaining data that it did not see during the training phase. The following code picks out a percentage of rows from $X$ randomly, together with corresponding entries of $\\vec{y}$, as the 'training set' and leaves the rest out as a 'testing set'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede219fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb8bce",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Now recall that we found an explicit formula for the optimal parameters in the multilinear regression problem. Let $\\vec{\\beta} = [\\beta_0 \\beta_1 \\cdots \\beta_d]^T$ be the optimal parameter vector. It is given by the formula\n",
    "$$\n",
    "\\vec{\\beta} = \\left(X^T X\\right)^{-1} X^T \\vec{y}.\n",
    "$$\n",
    "(In class, we wrote $\\vec{\\beta}^T = \\vec{y}^T X (X^T X)^{-1}$, which is equivalent to this formula.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984a68f",
   "metadata": {},
   "source": [
    "**Problem 2.** Find the optimal parameters for the training set using the formula above. Call your optimal parameter vector `betas`. Print out your answer. Note, the $X$ in the formula should be the `X_train` variable, and $\\vec{y}$ should be `y_train`.\n",
    "\n",
    "**Remark:** You need to figure out python code for the various matrix manipulations (transposes, inverting a matrix, etc.). Feel free to search the internet, use an LLM, and/or discuss the problem with classmates to figure out how to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a49e94",
   "metadata": {},
   "source": [
    "We can test the performance of our model by making predictions on the testing data, which was not incorporated into our training step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3082a",
   "metadata": {},
   "source": [
    "**Problem 3.** Create a vector `y_pred` containing the predicted `mpg` values of the data from your testing set.\n",
    "\n",
    "**Hint:** If $\\vec{\\beta}$ is the vector of parameters which minimizes the loss function, and $X_{test}$ is the data matrix for the test data, explain (to yourself) why the predicted labels should be given by $X_{test} \\cdot \\vec{\\beta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00714a",
   "metadata": {},
   "source": [
    "The code below plots the predicted `mpg` values on the test set, versus the true values from `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914cf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,5))\n",
    "plt.plot(y_pred,'o', label = 'Predicted')\n",
    "plt.plot(y_test,'x', label = 'Ground Truth')\n",
    "plt.title('Multilinear Regression Results')\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"MPG\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb396b-1515-4356-92ef-921dfc7b6cd0",
   "metadata": {},
   "source": [
    "The plot above is a bit hard to look at. The code below sorts the true values in increasing order, then sorts the predicted values with the same ordering. This makes it easier to see that our model is capturing the values of the data pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03b9dc-cba7-4757-92d4-0ec99d66b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_sorted = np.sort(y_test)\n",
    "y_pred_sorted = y_pred[np.argsort(y_test)]\n",
    "\n",
    "plt.figure(figsize = (7,5))\n",
    "plt.plot(y_pred_sorted,'o', label = 'Predicted')\n",
    "plt.plot(y_test_sorted,'x', label = 'Ground Truth')\n",
    "plt.title('Multilinear Regression Results')\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"MPG\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53498f85-60f1-41c2-831d-7256edb150cc",
   "metadata": {},
   "source": [
    "The results look reasonable, but to get a real feel for the quality of our results, we would want to test against other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5fef9",
   "metadata": {},
   "source": [
    "## Supervised Learning for Classification\n",
    "\n",
    "The multilinear regression model we created above is designed for predicting a continuous observation. Many machine learning models are designed to predict labels for labeled data (e.g., images). We will now explore some of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f469c",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "For this part of the notebook, we'll use the (small, built-in `sklearn` version of the) MNIST dataset of handwritten digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c884576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e8b6a",
   "metadata": {},
   "source": [
    "Recall that the data consists of many handwritten digits and their correct labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.imshow(digits.images[j], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c7d51",
   "metadata": {},
   "source": [
    "We can treat each image as a vector in $\\mathbb{R}^{8 \\times 8} \\approx \\mathbb{R}^{64}$. Conveniently, `digits.data` reshapes each image into a 64 dimensional vector. Let's create a train/test split of this vectorized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MNIST = digits.data\n",
    "MNISTlabels = digits.target\n",
    "\n",
    "MNIST_train, MNIST_test, MNISTlabels_train, MNISTlabels_test = train_test_split(MNIST, MNISTlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7398d",
   "metadata": {},
   "source": [
    "As we discussed in class, we can use Logistic Regression for image classification. The version that we discussed in class was only for classifying datasets with two labels ('cats' versus 'dogs', say), but this can be generalized to handle datasets with an arbitrary number of labels. In this case, we have 10 labels (one for each digit). \n",
    "\n",
    "The code below trains the logistic regression model on the training data (i.e., finds the best parameters for the model). This is standard `sklearn` syntax. Note that this training process uses a version of gradient descent to find optimal values, rather than an explicit formula as in the multilinear regression example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', multi_class = 'multinomial', max_iter=10000)\n",
    "model.fit(MNIST_train, MNISTlabels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423c7f6",
   "metadata": {},
   "source": [
    "Logistic regression with two labels works by finding a good hyperplane to split the two classes of data vectors (for precise definitions, refer back to the lecture notes). For multiclass logistic regression, we get a hyperplane for each class label. This can be seen in the shape of the coefficients for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44c3b1",
   "metadata": {},
   "source": [
    "Now we can predict a label for each digit in our testing set. This is done using the built-in `predict` function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1071c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_predicted = model.predict(MNIST_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c654e8",
   "metadata": {},
   "source": [
    "Let's take a look at some of the samples from our testing set, along with their predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "for j in range(20):\n",
    "    fig.add_subplot(4,5,j+1)\n",
    "    plt.imshow(MNIST_test[j].reshape(8,8), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Pred. Label='+str(MNIST_predicted[j]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ac5fa",
   "metadata": {},
   "source": [
    "These results look pretty good. We can compute the overall prediction accuracy using the `score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd94b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction Accuracy:',np.round(model.score(MNIST_test,MNISTlabels_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e039a",
   "metadata": {},
   "source": [
    "Logistic regression performs extremely well! It will probably be hard to beat this, but let's practice with some other models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88ad04",
   "metadata": {},
   "source": [
    "**Problem 4.** Repeat the above classification process using a different classification model. You can take your pick from [Multilayer Perceptron](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron) (a simple neural network), [Decision Tree](https://scikit-learn.org/stable/modules/tree.html#classification), [Support Vector Machine](https://scikit-learn.org/stable/modules/svm.html#classification), or any other model that you like.\n",
    "\n",
    "**Remark:** If you haven't done this before, you'll need to dig into the documentation a little bit to see how these work. The syntax is mostly similar to the logistic regression model. The mathematical details of several of these models will be described in final project presentations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
